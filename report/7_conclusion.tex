\chapter{Conclusion}
\label{conclusion}
We touched on a new task for citation analysis, Citation Provenance. While \outcite{csibs} presented the CSIBS tool that gave readers a preview of a cited paper, it does not provide information that justifies a citation. In our paper, we described the first attempt to provide a solution to the challenge of locating the origin of a citation

We presented a two-tier approach towards this problem, {\it GvS} and {\it LocateProv}. With the first acting as a filter to classify the citations into one of the two types: General and Specific. The second predicts which of the fragments in the cited paper are referenced by the citation. One of the difficulties in this task is the highly unbalanced ratio between General versus Specific citations. Also, the annotation task is very challenging and would require experienced researchers who understands the content of the papers to be annotated. As a result all the training instances were manually annotated.

To train prediction models for this task, we artificially sample an unskewed set of instances, a balanced ratio of General versus Specific instances, and measured their ability to differentiate between the 2 types of citations. Feature analysis showed that most of the features are essential, with the Physical Features (Feature $A$) adopted from \outcite{dongensemble} proving to have the most discriminative power in {\it GvS}, and Cosine Similarity (a common mechanism used in Information Retrieval tasks) remained to be most important in {\it LocateProv}.

Finally, evaluations on {\it GvS} and {\it LocateProv} produced promising results in classifying General versus Specific citations and locating the cited fragment in the cited paper. \textit{GvS} obtained an accuracy of 0.786 in our cross-validation, demonstrating its potential to perform in practice. \textit{LocateProv} showed a $25\%$ improvement compared to our baseline, justifying the features we introduced to target cited fragments.