\chapter{Evaluation}
\label{evaluation}
\paragraph{}
We performed 2 evaluations, one for each tier as described early in Chapter \ref{twotierapproach}. We are able to do this because the tiers are independent of each other.

%Tao: To make the results more complete, you may discuss the discriminative power of features by show the weights learned by machine learning algorithem. 
\section{Results - First Tier}
\paragraph{}
Recall that we have 275 annotated cite links, either General ($g$) or Specific ($s$), and that we have very limited instances of Specific cite links, a situation mentioned in \cite{li2010negative}, that we have a highly unbalanced ratio between General instances and Specific instances. So for our evaluation, we first gathered all Specific instances, and then \textbf{randomly} select General instances. Out of these 56 instances, we have $1:1$ ratio of Specific versus General instances. The reason for choosing a $1:1$ ratio is because we wish to measure our approach's ability to differentiate between General and Specific using a balanced training set.

%Tao: Give the reason of using Leave-One-Out evaluation
\paragraph{}
We trained our model using various classifiers, and then performed \url{Leave-One-Out} and \url{n-Fold} ($n=14$, each fold has 4 instances) evaluation using the 56 instances. The main reason for using \url{Leave-One-Out} is because we are working with limited instances we wish to maximise them for training. As for the performance when given less training data, we use the \url{n-Fold} strategy. We did this for the various classifiers and Table \ref{tab:firsttieresults} summarises the performance for each classifier.

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c  c }
		\textsc{Classifier/Values} & \textsc{Avg. Precision} & \textsc{Avg. Recall} & \textsc{Avg. F$_1$-Score} \\
		\hline
		\textsc{SVM} 			& 0.75 & 0.75 & 0.75 \\
		\textsc{NaiveBayes} 	& 0.67 & 0.64 & 0.63 \\
		\textsc{DecisionTree}	& 0.68 & 0.68 & 0.68
	\end{tabular}
	\caption{First Tier Results}
	\label{tab:firsttieresults}
\end{table}

\paragraph{}
We examine the confusion matrix for the SVM classifier (see Table \ref{tab:svmconfusionmatrix}). We can observe that our First Tier has done considerably well in differentiating General and Specific cite links.

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c }
		 & \textsc{actual $g$} & \textsc{actual $s$} \\
		\hline
		\textsc{predicted $g$} 	& 20 & 8 \\
		\textsc{predicted $s$}		& 6 & 22
	\end{tabular}
	\caption{Confusion Matrix for SVM}
	\label{tab:svmconfusionmatrix}
\end{table}

\section{Results - Second Tier}
\paragraph{}
For Second Tier evaluation, we are predicting whether each fragment in the cited paper is a Specific one. We have over 30 thousand training instances for second tier, and so for the same reason, we had to select our training set manually similarly to get a $1:1$ ratio for Specific versus General instances.

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c  c }
		\textsc{Classifier/Values} & \textsc{Avg. Precision} & \textsc{Avg. Recall} & \textsc{Avg. F$_1$-Score} \\
		\hline
		\textsc{SVM} 			& 0.84 & 0.82 & 0.82 \\
		\textsc{NaiveBayes} 	& 0.84 & 0.84 & 0.84 \\
		\textsc{DecisionTree}	& 0.80 & 0.80 & 0.80
	\end{tabular}
	\caption{Second Tier Results}
	\label{tab:secondtieresults}
\end{table}
\newpage
\paragraph{}
Now if we look at Table \ref{tab:naivebayesconfusionmatrix} for the confusion matrix for the Naive Bayes classifier, we can observe that the features we defined for the Second Tier performed reasonably well in predicting the class of the citation. 

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c }
		 & \textsc{actual $g$} & \textsc{actual $s$} \\
		\hline
		\textsc{predicted $g$} 	& 24 & 4 \\
		\textsc{predicted $s$}		& 5 & 23
	\end{tabular}
	\caption{Confusion Matrix for Naive Bayes}
	\label{tab:naivebayesconfusionmatrix}
\end{table}

(Refer to Appendix \ref{resultsdetails} for more details of our experimental results.)