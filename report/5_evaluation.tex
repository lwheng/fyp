\chapter{Evaluation}
\label{evaluation}
I performed modular evaluation on {\it GvS} and {\it LocateProv}. For each tier I evaluated its performance on a few classifiers: Support Vector Machine (SVM), Naive Bayes (NB) and Decision Tree (DT). For each classifier I also performed evaluation using a few evaluation strategies.

\section{Evaluating {\it GvS}}
\label{eval:first}
Recall that I used a $1:1$ of Specific versus General data instances for building the model. To first verify {\it GvS}, I evaluated the features added using the {\it feature ablation} strategy. For each feature removed from this set of unskewed data instances, the rest of the features are used to train a model using the SVM classifier and then tested on the same set of data instances. To measure the performance each round, I used the conventional accuracy measure. Note that in Figure \ref{fig:ablation_first} the letters $A$ to $E$ represents the five features described in Chapter \ref{firsttier}.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}\centering
\begin{tabular}{ l | l }
Configuration & Accuracy \\
\hline
Full		& 0.911 \\
Full - A	& 0.714 \\
Full - B	& 0.875 \\
Full - C	& 0.786 \\
Full - D	& 0.911 \\
Full - E	& 0.732 \\
\end{tabular}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}\centering
\begin{tabular}{ c | l }
Configuration & Accuracy \\
\hline
Only A	& 0.696 \\
Only B	& 0.589 \\
Only C	& 0.625 \\
Only D	& 0.535 \\
Only E	& 0.696 \\
\end{tabular}
\end{minipage}
\caption{Feature Ablation on {\it GvS}}
\label{fig:ablation_first}
\end{figure}

We observe that feature $A$ (Physical Feature) has the most impact in the accuracy of the predictions, with the greatest drop in accuracy when $A$ itself is removed and one of the highest accuracy when $A$ alone is used as a feature (see Figure \ref{fig:ablation_first}). Feature $D$ (Citing Context's Average \url{tf-idf} Weight) appears to be the only redundant feature, but since it does not decrease the overall accuracy we shall include it nevertheless.

We continue evaluating {\it GvS} using the \url{Leave-One-Out} strategy. In this strategy we leave one data instance out for testing while the rest are used for training and we repeat this for the number of instances. The main reason for using this strategy is because the number of data instances in the unskewed data set is already very small, and I wish to maximise them for training. For this strategy I compare the performance of the various classifiers, for each, computing the Precision, Recall and F$_1$ values.

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c  c | c c c | c c c}
		& & SVM & & & NB & & & DT \\
		\textsc{Class/Values} & \textsc{P} & \textsc{R} & \textsc{F$_1$} & \textsc{P} & \textsc{R} & \textsc{F$_1$} & \textsc{P} & \textsc{R} & \textsc{F$_1$} \\
		\hline
		\textsc{general} 			& 0.76  &    0.79   &   0.77 & 0.64   &   0.82   &   0.72 & 0.67  &    0.64  &    0.65 \\
		\textsc{specific} 			& 0.78  &    0.75   &   0.76 & 0.75   &   0.54   &   0.63 & 0.66  &    0.68  &    0.67 \\
	\end{tabular}
	\caption{Leave-One-Out Results for {\it GvS}}
	\label{tab:firsttieresults}
\end{table}

We examine the confusion matrix for the best performing SVM classifier that we ran for the \url{Leave-One-Out} strategy. Recall that {\it GvS}'s objective is to filter out the General citations. Our goal is to attain higher numbers in both the $g$-$g$ and $s$-$s$ cells in the confusion matrix. We achieved this in Table \ref{tab:svmconfusionmatrix} and we can conclude that {\it GvS} has a promising performance in differentiating General and Specific citations.

\begin{table}[h]
	\center
	\begin{tabular}{ c | c  c }
		 & \textsc{actual $g$} & \textsc{actual $s$} \\
		\hline
		\textsc{predicted $g$} 	& 22 & 6 \\
		\textsc{predicted $s$}		& 7 & 21
	\end{tabular}
	\caption{Confusion Matrix for SVM with Leave-One-Out on {\it GvS}}
	\label{tab:svmconfusionmatrix}
\end{table}

For a more conclusive evaluation, I continue evaluating {\it GvS} by training on the unskewed data set, and then testing on the entire data set.

\section{Evaluating {\it LocateProv}}
\label{eval:second}
Similar to evaluating {\it GvS} in Chapter \ref{eval:first}, I first evaluate the features added to {\it LocateProv} using the {\it feature ablation} strategy. Note that the letters $F$ to $I$ represents the features described in Chapter \ref{secondtier}.

\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}\centering
\begin{tabular}{ l | l }
Configuration & Accuracy \\
\hline
Full		& 0.893 \\
Full - F	& 0.893 \\
Full - G	& 0.875 \\
Full - H	& 0.893 \\
Full - I	& 0.786 \\
\end{tabular}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}\centering
\begin{tabular}{ c | l }
Configuration & Accuracy \\
\hline
Only F	& 0.714 \\
Only G	& 0.625 \\
Only H	& 0.607 \\
Only I	& 0.875 \\
\end{tabular}
\end{minipage}
\caption{Feature Ablation on {\it LocateProv}}
\label{fig:ablation_second}
\end{figure}
From Figure \ref{fig:ablation_second} we can conclude that feature $I$ (Cosine Similarity) remains to be the most important among the features for {\it LocateProv}. This is expected because as modelled in Chapter \ref{problemanalysis}, {\it LocateProv} is a searching problem, thus an Information Retrieval solution is most suitable.

%For Second Tier evaluation, we are predicting whether each fragment in the cited paper is a Specific one. We have over 30 thousand training instances for second tier, and so for the same reason, we had to select our training set manually similarly to get a $1:1$ ratio for Specific versus General instances.
%
%\begin{table}[h]
%	\center
%	\begin{tabular}{ c | c  c  c }
%		\textsc{Classifier/Values} & \textsc{Avg. Precision} & \textsc{Avg. Recall} & \textsc{Avg. F$_1$-Score} \\
%		& \url{LOO} / \url{n-Fold} & \url{LOO} / \url{n-Fold} & \url{LOO} / \url{n-Fold} \\
%		\hline
%		\textsc{SVM} 			& 0.85 / 0.84 & 0.84 / 0.82 & 0.84 / 0.82 \\
%		\textsc{NaiveBayes} 	& 0.80 / 0.78 & 0.79 / 0.77 & 0.78 / 0.77 \\
%		\textsc{DecisionTree}	& 0.89 / 0.86 & 0.89 / 0.86 & 0.89 / 0.86
%	\end{tabular}
%	\caption{Second Tier Results}
%	\label{tab:secondtieresults}
%\end{table}
%\newpage
%In this case, the Decision Tree classifier performed slightly better than SVM. Similarly, our goal is to attain higher numbers in both the $g$-$g$ and $s$-$s$ cells in the confusion matrix and in Table \ref{tab:decisiontreeconfusionmatrix} we achieved good results. This means, given a Specific citation, this approach would perform well in determining whether a fragment in the cited paper is the cited fragment or not.
%
%\begin{table}[h]
%	\center
%	\begin{tabular}{ c | c  c }
%		 & \textsc{actual $g$} & \textsc{actual $s$} \\
%		\hline
%		\textsc{predicted $g$} 	& 23 & 5 \\
%		\textsc{predicted $s$}		& 3 & 25
%	\end{tabular}
%	\caption{Confusion Matrix for Naive Bayes}
%	\label{tab:decisiontreeconfusionmatrix}
%\end{table}
%
%(Refer to Appendix \ref{resultsdetails} for more details of our experimental results.)